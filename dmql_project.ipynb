{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f12e0ef-a178-4195-b804-33de32ab0658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading faker-36.2.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from faker) (2023.3)\n",
      "Downloading faker-36.2.2-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faker\n",
      "Successfully installed faker-36.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5356d8d-76fe-4573-999a-1e29a147bf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been successfully created and are ready for direct database insertion!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from faker import Faker\n",
    "\n",
    "def extract_and_generate_csv():\n",
    "    fake = Faker()\n",
    "    \n",
    "    # Load the original CSV files\n",
    "    movies_path = \"tmdb_5000_movies.csv\"\n",
    "    theaters_path = \"theaters.csv\"\n",
    "    \n",
    "    movies_df = pd.read_csv(movies_path)\n",
    "    theaters_df = pd.read_csv(theaters_path)\n",
    "    \n",
    "    # Generate new unique IDs for movies and theatres\n",
    "    movies_df['movie_id'] = range(1000, 1000 + len(movies_df))\n",
    "    theaters_df['theatre_id'] = range(5000, 5000 + len(theaters_df))\n",
    "    \n",
    "    # Extract relevant columns from Movies dataset\n",
    "    def extract_production_house(production_companies):\n",
    "        try:\n",
    "            companies = json.loads(production_companies.replace(\"'\", \"\\\"\"))\n",
    "            return companies[0]['name'] if companies else fake.company()\n",
    "        except:\n",
    "            return fake.company()\n",
    "    \n",
    "    movies_selected = movies_df[['movie_id', 'title', 'vote_average', 'release_date', 'runtime', 'production_companies']].copy()\n",
    "    movies_selected = movies_selected.drop(columns=[\"production_companies\"])\n",
    "    movies_selected.rename(columns={\n",
    "        'title': 'name',\n",
    "        'vote_average': 'rating',\n",
    "        'release_date': 'release_date',\n",
    "        'runtime': 'run_time'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    movies_selected['production_house'] = movies_df['production_companies'].apply(extract_production_house)\n",
    "    \n",
    "    # Fill missing values with fake data\n",
    "    movies_selected.fillna({\n",
    "        'rating': round(random.uniform(5.0, 9.0), 1),\n",
    "        'release_date': fake.date(),\n",
    "        'run_time': random.randint(80, 180)\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Extract relevant columns from Theatres dataset\n",
    "    theaters_selected = theaters_df[['theatre_id', 'name', 'address', 'city', 'state', 'zip', 'lat', 'lon']].copy()\n",
    "    theaters_selected.rename(columns={\n",
    "        'name': 'theatre_name',\n",
    "        'lat': 'latitude',\n",
    "        'lon': 'longitude'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Generate random screens data (max 20 per theatre)\n",
    "    screens_data = []\n",
    "    for _, row in theaters_selected.iterrows():\n",
    "        num_screens = random.randint(5, 20)\n",
    "        for screen_no in range(1, num_screens + 1):\n",
    "            screens_data.append({\n",
    "                'screen_id': f\"{row['theatre_id']}_{screen_no}\",\n",
    "                'theatre_id': row['theatre_id'],\n",
    "                'screen_no': screen_no,\n",
    "                'screen_type': random.choice(['IMAX', '3D', 'Standard', '4DX', 'Dolby Cinema']),\n",
    "                'capacity': random.randint(50, 300)\n",
    "            })\n",
    "    \n",
    "    screens_df = pd.DataFrame(screens_data)\n",
    "    \n",
    "    # Create Screening Schedule\n",
    "    schedule_data = []\n",
    "    for _, screen_row in screens_df.iterrows():\n",
    "        num_movies = random.randint(1, 3)  # Each screen can play up to 3 different movies a day\n",
    "        movies_sample = movies_selected.sample(num_movies)  # Randomly select movies for this screen\n",
    "        for _, movie_row in movies_sample.iterrows():\n",
    "            show_times = ['10:00 AM', '01:00 PM', '04:00 PM', '07:00 PM', '10:00 PM']\n",
    "            for show_time in random.sample(show_times, random.randint(2, 4)):  # Assign 2-4 random showtimes\n",
    "                schedule_data.append({\n",
    "                    'schedule_id': f\"{screen_row['screen_id']}_{movie_row['movie_id']}_{show_time.replace(':', '')}\",\n",
    "                    'screen_id': screen_row['screen_id'],\n",
    "                    'movie_id': movie_row['movie_id'],\n",
    "                    'show_date': fake.date_this_year().strftime('%Y-%m-%d'),  # Random show date\n",
    "                    'show_time': show_time,\n",
    "                    'available_seats': screen_row['capacity']\n",
    "                })\n",
    "    \n",
    "    schedule_df = pd.DataFrame(schedule_data)\n",
    "    \n",
    "    # Save extracted data to CSV files\n",
    "    movies_selected.to_csv(\"Movies.csv\", index=False)\n",
    "    theaters_selected.to_csv(\"Theatres.csv\", index=False)\n",
    "    screens_df.to_csv(\"Screens.csv\", index=False)\n",
    "    schedule_df.to_csv(\"Screening_Schedule.csv\", index=False)\n",
    "    \n",
    "    print(\"CSV files have been successfully created and are ready for direct database insertion!\")\n",
    "\n",
    "# Run the function\n",
    "extract_and_generate_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "111e5598-0d72-42f3-b33d-241372abcbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Transactions CSV file has been successfully created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "def generate_customer_transactions():\n",
    "    fake = Faker()\n",
    "    \n",
    "    # Load the generated CSV files\n",
    "    movies_df = pd.read_csv(\"Movies.csv\")\n",
    "    theaters_df = pd.read_csv(\"Theatres.csv\")\n",
    "    screens_df = pd.read_csv(\"Screens.csv\")\n",
    "    schedule_df = pd.read_csv(\"Screening_Schedule.csv\")\n",
    "    \n",
    "    # Generate customer transactions\n",
    "    transactions_data = []\n",
    "    num_transactions = 100000 \n",
    "    \n",
    "    for _ in range(num_transactions):\n",
    "        schedule_sample = schedule_df.sample(1).iloc[0]  # Select a random screening\n",
    "        seats_booked = random.randint(1, 5)  # Random number of seats booked\n",
    "        ticket_price = random.choice([10, 12, 15, 20])  # Random ticket price\n",
    "        food_amount = random.choice([0, 5, 10, 15, 20])  # Food purchase amount\n",
    "        total_amount = (seats_booked * ticket_price) + food_amount\n",
    "        \n",
    "        transactions_data.append({\n",
    "            'transaction_id': fake.uuid4(),\n",
    "            'customer_name': fake.name(),\n",
    "            'email_id': fake.email(),\n",
    "            'promo_code_used': random.choice([None, 'DISCOUNT10', 'SUMMER20', 'MOVIE50']),\n",
    "            'schedule_id': schedule_sample['schedule_id'],\n",
    "            'seats_booked': seats_booked,\n",
    "            'ticket_price': ticket_price,\n",
    "            'food_bought': random.choice([None, 'Popcorn', 'Soda', 'Nachos', 'Hotdog']),\n",
    "            'food_amount': food_amount,\n",
    "            'total_amount': total_amount,\n",
    "            'booking_date': fake.date_this_year().strftime('%Y-%m-%d')\n",
    "        })\n",
    "    \n",
    "    transactions_df = pd.DataFrame(transactions_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    transactions_df.to_csv(\"Customer_Transactions.csv\", index=False)\n",
    "    \n",
    "    print(\"Customer Transactions CSV file has been successfully created!\")\n",
    "\n",
    "# Run the function\n",
    "generate_customer_transactions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5920351e-e4d2-4d00-8a3b-5bd32dbc76e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theatre Staff and Daily Shifts CSV files have been successfully created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "def generate_theatre_staff_and_shifts():\n",
    "    fake = Faker()\n",
    "    \n",
    "    # Load the generated Theatres CSV file\n",
    "    theaters_df = pd.read_csv(\"Theatres.csv\")\n",
    "    \n",
    "    # Generate Theatre Staff\n",
    "    staff_data = []\n",
    "    roles = ['Manager', 'Cashier', 'Usher', 'Cleaner', 'Security', 'Projectionist']\n",
    "    \n",
    "    staff_id_counter = 1\n",
    "    for _, theatre in theaters_df.iterrows():\n",
    "        num_staff = random.randint(5, 15)  # Random number of staff per theatre\n",
    "        for _ in range(num_staff):\n",
    "            staff_data.append({\n",
    "                'staff_id': staff_id_counter,\n",
    "                'theatre_id': theatre['theatre_id'],\n",
    "                'name': fake.name(),\n",
    "                'role': random.choice(roles),\n",
    "                'salary': round(random.uniform(25000, 70000), 2),\n",
    "                'contact_no': fake.phone_number(),\n",
    "                'email': fake.email()\n",
    "            })\n",
    "            staff_id_counter += 1\n",
    "    \n",
    "    staff_df = pd.DataFrame(staff_data)\n",
    "    \n",
    "    # Generate Daily Shifts\n",
    "    shifts_data = []\n",
    "    shift_id_counter = 1\n",
    "    \n",
    "    for _, staff in staff_df.iterrows():\n",
    "        num_shifts = random.randint(20, 30)  # Each staff works around 20-30 shifts per month\n",
    "        for _ in range(num_shifts):\n",
    "            shifts_data.append({\n",
    "                'shift_id': shift_id_counter,\n",
    "                'staff_id': staff['staff_id'],\n",
    "                'shift_date': fake.date_this_year().strftime('%Y-%m-%d'),\n",
    "                'shift_start_time': fake.time(pattern=\"%H:%M:%S\"),\n",
    "                'shift_end_time': fake.time(pattern=\"%H:%M:%S\")\n",
    "            })\n",
    "            shift_id_counter += 1\n",
    "    \n",
    "    shifts_df = pd.DataFrame(shifts_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    staff_df.to_csv(\"Theatre_Staff.csv\", index=False)\n",
    "    shifts_df.to_csv(\"Daily_Shifts.csv\", index=False)\n",
    "    \n",
    "    print(\"Theatre Staff and Daily Shifts CSV files have been successfully created!\")\n",
    "\n",
    "# Run the function\n",
    "generate_theatre_staff_and_shifts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91b14000-6d1c-48b7-956d-6e12d79763c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tables Customers, Transactions, Food_Details, and Promotions have been created and data inserted!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "def normalize_and_insert():\n",
    "    fake = Faker()\n",
    "    \n",
    "    # Load existing datasets\n",
    "    movies_df = pd.read_csv(\"Movies.csv\")\n",
    "    theaters_df = pd.read_csv(\"Theatres.csv\")\n",
    "    screens_df = pd.read_csv(\"Screens.csv\")\n",
    "    schedule_df = pd.read_csv(\"Screening_Schedule.csv\")\n",
    "    transactions_df = pd.read_csv(\"Customer_Transactions.csv\")\n",
    "    staff_df = pd.read_csv(\"Theatre_Staff.csv\")\n",
    "    shifts_df = pd.read_csv(\"Daily_Shifts.csv\")\n",
    "    \n",
    "    # 1. Customer Table (Separate customer details)\n",
    "    customer_data = {}\n",
    "    for _, transaction in transactions_df.iterrows():\n",
    "        if transaction['email_id'] not in customer_data:\n",
    "            customer_data[transaction['email_id']] = {\n",
    "                'customer_id': len(customer_data) + 1,\n",
    "                'customer_name': transaction['customer_name'],\n",
    "                'email_id': transaction['email_id'],\n",
    "                'contact_no': fake.phone_number()\n",
    "            }\n",
    "    customer_df = pd.DataFrame(customer_data.values())\n",
    "    \n",
    "    # 2. Transaction Table (Link customers to transactions separately)\n",
    "    transaction_data = []\n",
    "    for _, transaction in transactions_df.iterrows():\n",
    "        customer_id = customer_df.loc[customer_df['email_id'] == transaction['email_id'], 'customer_id'].values[0]\n",
    "        transaction_data.append({\n",
    "            'transaction_id': transaction['transaction_id'],\n",
    "            'customer_id': customer_id,\n",
    "            'schedule_id': transaction['schedule_id'],\n",
    "            'seats_booked': transaction['seats_booked'],\n",
    "            'ticket_price': transaction['ticket_price'],\n",
    "            'food_amount': transaction['food_amount'],\n",
    "            'total_amount': transaction['total_amount'],\n",
    "            'booking_date': transaction['booking_date']\n",
    "        })\n",
    "    transactions_normalized_df = pd.DataFrame(transaction_data)\n",
    "    \n",
    "    # 3. Food Details (Break down food purchases per transaction)\n",
    "    food_items = ['Popcorn', 'Soda', 'Nachos', 'Hotdog', 'Burger', 'Candy']\n",
    "    food_details_data = []\n",
    "    for _, transaction in transactions_df.iterrows():\n",
    "        if pd.notna(transaction['food_bought']):\n",
    "            for food in transaction['food_bought'].split(', '):\n",
    "                food_details_data.append({\n",
    "                    'transaction_id': transaction['transaction_id'],\n",
    "                    'food_name': food,\n",
    "                    'price': round(random.uniform(3.0, 10.0), 2)\n",
    "                })\n",
    "    food_details_df = pd.DataFrame(food_details_data)\n",
    "    \n",
    "    # 4. Promotions Table (Store promo codes and discounts separately)\n",
    "    promo_codes = ['DISCOUNT10', 'SUMMER20', 'MOVIE50', None]\n",
    "    promotions_data = []\n",
    "    for _, transaction in transactions_df.iterrows():\n",
    "        if transaction['promo_code_used'] in promo_codes[:-1]:  # Exclude None values\n",
    "            promotions_data.append({\n",
    "                'transaction_id': transaction['transaction_id'],\n",
    "                'promo_code': transaction['promo_code_used'],\n",
    "                'discount_applied': round(random.uniform(5.0, 20.0), 2)\n",
    "            })\n",
    "    promotions_df = pd.DataFrame(promotions_data)\n",
    "    \n",
    "    # Save new tables\n",
    "    customer_df.to_csv(\"Customers.csv\", index=False)\n",
    "    transactions_normalized_df.to_csv(\"Transactions.csv\", index=False)\n",
    "    food_details_df.to_csv(\"Food_Details.csv\", index=False)\n",
    "    promotions_df.to_csv(\"Promotions.csv\", index=False)\n",
    "    \n",
    "    print(\"New tables Customers, Transactions, Food_Details, and Promotions have been created and data inserted!\")\n",
    "\n",
    "# Run the function\n",
    "normalize_and_insert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5b2a223-eb6b-496d-93ae-0daa2e24bd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.10.tar.gz (385 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.7/385.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2: filename=psycopg2-2.9.10-cp312-cp312-macosx_11_0_arm64.whl size=133409 sha256=8903b8f3db086689d66b4c013aedd139f85fab8907c5e7a683f5ee45d630d355\n",
      "  Stored in directory: /Users/vamsisaigarapati/Library/Caches/pip/wheels/ac/bb/ce/afa589c50b6004d3a06fc691e71bd09c9bd5f01e5921e5329b\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.10\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8497bd90-300f-4d94-a516-bf63e53055f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 2003 (HY000): Can't connect to MySQL server on 'localhost:3306' (61)\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "import psycopg2\n",
    "def create_database_and_tables():\n",
    "    try:\n",
    "        conn = psycopg2.connect( # Database name\n",
    "    user=\"postgres\",  # Your username\n",
    "    password=\"NbKhArDy369#\",  # Your password\n",
    "    host=\"localhost\",  # Or the host of your database\n",
    "    port=\"5432\"  # Or the port if it's different\n",
    ")\n",
    "        # Connect to MySQL Server\n",
    "        connection = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            user='postgres',\n",
    "            password='NbKhArDy369@'\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Create Database\n",
    "        cursor.execute(\"CREATE DATABASE IF NOT EXISTS MovieChainDB\")\n",
    "        cursor.execute(\"USE MovieChainDB\")\n",
    "        \n",
    "        # Define Table Creation Queries\n",
    "        TABLES = {}\n",
    "        TABLES['Movies'] = (\n",
    "            \"CREATE TABLE IF NOT EXISTS Movies (\"\n",
    "            \"  movie_id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "            \"  name VARCHAR(255) NOT NULL,\"\n",
    "            \"  rating DECIMAL(3,1) NOT NULL,\"\n",
    "            \"  production_house VARCHAR(255) NOT NULL,\"\n",
    "            \"  release_date DATE NOT NULL,\"\n",
    "            \"  run_time INT NOT NULL CHECK (run_time > 0)\"\n",
    "            \")\"\n",
    "        )\n",
    "        \n",
    "        TABLES['Theatres'] = (\n",
    "            \"CREATE TABLE IF NOT EXISTS Theatres (\"\n",
    "            \"  theatre_id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "            \"  name VARCHAR(255) NOT NULL,\"\n",
    "            \"  address VARCHAR(255) NOT NULL,\"\n",
    "            \"  city VARCHAR(100) NOT NULL,\"\n",
    "            \"  state VARCHAR(50) NOT NULL,\"\n",
    "            \"  zip VARCHAR(20) NOT NULL,\"\n",
    "            \"  language VARCHAR(50) NOT NULL,\"\n",
    "            \"  latitude DECIMAL(10,6) NOT NULL,\"\n",
    "            \"  longitude DECIMAL(10,6) NOT NULL\"\n",
    "            \")\"\n",
    "        )\n",
    "        \n",
    "        TABLES['Screens'] = (\n",
    "            \"CREATE TABLE IF NOT EXISTS Screens (\"\n",
    "            \"  screen_id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "            \"  theatre_id INT NOT NULL,\"\n",
    "            \"  screen_no INT NOT NULL,\"\n",
    "            \"  screen_type VARCHAR(50) NOT NULL,\"\n",
    "            \"  capacity INT NOT NULL CHECK (capacity > 0),\"\n",
    "            \"  FOREIGN KEY (theatre_id) REFERENCES Theatres(theatre_id) ON DELETE CASCADE\"\n",
    "            \")\"\n",
    "        )\n",
    "        \n",
    "        TABLES['Screening_Schedule'] = (\n",
    "            \"CREATE TABLE IF NOT EXISTS Screening_Schedule (\"\n",
    "            \"  schedule_id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "            \"  screen_id INT NOT NULL,\"\n",
    "            \"  movie_id INT NOT NULL,\"\n",
    "            \"  show_date DATE NOT NULL,\"\n",
    "            \"  show_time TIME NOT NULL,\"\n",
    "            \"  available_seats INT NOT NULL CHECK (available_seats >= 0),\"\n",
    "            \"  FOREIGN KEY (screen_id) REFERENCES Screens(screen_id) ON DELETE CASCADE,\"\n",
    "            \"  FOREIGN KEY (movie_id) REFERENCES Movies(movie_id) ON DELETE CASCADE\"\n",
    "            \")\"\n",
    "        )\n",
    "        \n",
    "        TABLES['Customers'] = (\n",
    "            \"CREATE TABLE IF NOT EXISTS Customers (\"\n",
    "            \"  customer_id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "            \"  customer_name VARCHAR(255) NOT NULL,\"\n",
    "            \"  email_id VARCHAR(255) UNIQUE NOT NULL,\"\n",
    "            \"  contact_no VARCHAR(20) NOT NULL\"\n",
    "            \")\"\n",
    "        )\n",
    "        \n",
    "        TABLES['Transactions'] = (\n",
    "            \"CREATE TABLE IF NOT EXISTS Transactions (\"\n",
    "            \"  transaction_id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "            \"  customer_id INT NOT NULL,\"\n",
    "            \"  schedule_id INT NOT NULL,\"\n",
    "            \"  seats_booked INT NOT NULL CHECK (seats_booked > 0),\"\n",
    "            \"  ticket_price DECIMAL(5,2) NOT NULL,\"\n",
    "            \"  food_amount DECIMAL(5,2) NOT NULL DEFAULT 0,\"\n",
    "            \"  total_amount DECIMAL(6,2) NOT NULL,\"\n",
    "            \"  booking_date DATETIME NOT NULL,\"\n",
    "            \"  FOREIGN KEY (customer_id) REFERENCES Customers(customer_id) ON DELETE CASCADE,\"\n",
    "            \"  FOREIGN KEY (schedule_id) REFERENCES Screening_Schedule(schedule_id) ON DELETE CASCADE\"\n",
    "            \")\"\n",
    "        )\n",
    "        \n",
    "        TABLES['Food_Details'] = (\n",
    "            \"CREATE TABLE IF NOT EXISTS Food_Details (\"\n",
    "            \"  food_id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "            \"  transaction_id INT NOT NULL,\"\n",
    "            \"  food_name VARCHAR(100) NOT NULL,\"\n",
    "            \"  price DECIMAL(5,2) NOT NULL CHECK (price > 0),\"\n",
    "            \"  FOREIGN KEY (transaction_id) REFERENCES Transactions(transaction_id) ON DELETE CASCADE\"\n",
    "            \")\"\n",
    "        )\n",
    "        \n",
    "        TABLES['Promotions'] = (\n",
    "            \"CREATE TABLE IF NOT EXISTS Promotions (\"\n",
    "            \"  promo_id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "            \"  transaction_id INT NOT NULL,\"\n",
    "            \"  promo_code VARCHAR(50) NOT NULL,\"\n",
    "            \"  discount_applied DECIMAL(5,2) NOT NULL CHECK (discount_applied > 0),\"\n",
    "            \"  FOREIGN KEY (transaction_id) REFERENCES Transactions(transaction_id) ON DELETE CASCADE\"\n",
    "            \")\"\n",
    "        )\n",
    "        \n",
    "        TABLES['Theatre_Staff'] = (\n",
    "            \"CREATE TABLE IF NOT EXISTS Theatre_Staff (\"\n",
    "            \"  staff_id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "            \"  theatre_id INT NOT NULL,\"\n",
    "            \"  name VARCHAR(255) NOT NULL,\"\n",
    "            \"  role VARCHAR(50) NOT NULL,\"\n",
    "            \"  salary DECIMAL(8,2) NOT NULL CHECK (salary > 0),\"\n",
    "            \"  contact_no VARCHAR(20) NOT NULL,\"\n",
    "            \"  email VARCHAR(255) UNIQUE NOT NULL,\"\n",
    "            \"  FOREIGN KEY (theatre_id) REFERENCES Theatres(theatre_id) ON DELETE CASCADE\"\n",
    "            \")\"\n",
    "        )\n",
    "        \n",
    "        TABLES['Daily_Shifts'] = (\n",
    "            \"CREATE TABLE IF NOT EXISTS Daily_Shifts (\"\n",
    "            \"  shift_id INT AUTO_INCREMENT PRIMARY KEY,\"\n",
    "            \"  staff_id INT NOT NULL,\"\n",
    "            \"  shift_date DATE NOT NULL,\"\n",
    "            \"  shift_start_time TIME NOT NULL,\"\n",
    "            \"  shift_end_time TIME NOT NULL,\"\n",
    "            \"  FOREIGN KEY (staff_id) REFERENCES Theatre_Staff(staff_id) ON DELETE CASCADE\"\n",
    "            \")\"\n",
    "        )\n",
    "        \n",
    "        # Execute table creation queries\n",
    "        for table_name, table_query in TABLES.items():\n",
    "            cursor.execute(table_query)\n",
    "        \n",
    "        print(\"Database and tables created successfully!\")\n",
    "        \n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        \n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "\n",
    "# Run the function\n",
    "create_database_and_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf219825-c49b-45c6-b3d8-f83752bc9d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL Server 1 successfully!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connection details for Server 1\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"NbKhArDy369@\"\n",
    "DB_HOST = \"localhost\"  # Change to the correct host IP\n",
    "DB_PORT = \"5432\"  # Default PostgreSQL port\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    connection = psycopg2.connect(\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT\n",
    "    )\n",
    "    print(\"Connected to PostgreSQL Server 1 successfully!\")\n",
    "\n",
    "    # Close connection\n",
    "    connection.close()\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Error connecting to PostgreSQL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1fb133d-db40-465c-94af-d269ab3da0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'movie_chain_db3' created successfully!\n",
      "All tables created successfully!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "def create_database():\n",
    "    try:\n",
    "        # Connect to PostgreSQL server (default 'postgres' database)\n",
    "        connection = psycopg2.connect(\n",
    "            dbname='postgres',  # Connect to the default database first\n",
    "            user='postgres',\n",
    "            password='NbKhArDy369@',\n",
    "            host='localhost',  # Change if connecting to a remote server\n",
    "            port='5432'\n",
    "        )\n",
    "        connection.autocommit = True  # Required for creating databases\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Create the database if it does not exist\n",
    "        cursor.execute(\"CREATE DATABASE movie_chain_db3;\")\n",
    "        print(\"Database 'movie_chain_db3' created successfully!\")\n",
    "        \n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error creating database: {e}\")\n",
    "\n",
    "def create_tables():\n",
    "    try:\n",
    "        # Connect to the new database\n",
    "        connection = psycopg2.connect(\n",
    "            dbname='movie_chain_db3',\n",
    "            user='postgres',\n",
    "            password='NbKhArDy369@',\n",
    "            host='localhost',\n",
    "            port='5432'\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # List of SQL create table statements\n",
    "        TABLES = [\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS Movies (\n",
    "                movie_id SERIAL PRIMARY KEY,\n",
    "                name VARCHAR(255) NOT NULL,\n",
    "                rating DECIMAL(3,1) NOT NULL,\n",
    "                production_house VARCHAR(255) NOT NULL,\n",
    "                release_date DATE NOT NULL,\n",
    "                run_time INT NOT NULL CHECK (run_time > 0)\n",
    "            )\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS Theatres (\n",
    "                theatre_id SERIAL PRIMARY KEY,\n",
    "                name VARCHAR(255) NOT NULL,\n",
    "                address VARCHAR(255) NOT NULL,\n",
    "                city VARCHAR(100) NOT NULL,\n",
    "                state VARCHAR(50) NOT NULL,\n",
    "                zip VARCHAR(20) NOT NULL,\n",
    "                latitude DECIMAL(10,6) NOT NULL,\n",
    "                longitude DECIMAL(10,6) NOT NULL\n",
    "            )\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS Screens (\n",
    "                screen_id SERIAL PRIMARY KEY,\n",
    "                theatre_id INT NOT NULL,\n",
    "                screen_no INT NOT NULL,\n",
    "                screen_type VARCHAR(50) NOT NULL,\n",
    "                capacity INT NOT NULL CHECK (capacity > 0),\n",
    "                FOREIGN KEY (theatre_id) REFERENCES Theatres(theatre_id) ON DELETE CASCADE\n",
    "            )\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS Screening_Schedule (\n",
    "                schedule_id SERIAL PRIMARY KEY,\n",
    "                screen_id INT NOT NULL,\n",
    "                movie_id INT NOT NULL,\n",
    "                show_date DATE NOT NULL,\n",
    "                show_time TIME NOT NULL,\n",
    "                available_seats INT NOT NULL CHECK (available_seats >= 0),\n",
    "                FOREIGN KEY (screen_id) REFERENCES Screens(screen_id) ON DELETE CASCADE,\n",
    "                FOREIGN KEY (movie_id) REFERENCES Movies(movie_id) ON DELETE CASCADE\n",
    "            )\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS Customers (\n",
    "                customer_id SERIAL PRIMARY KEY,\n",
    "                customer_name VARCHAR(255) NOT NULL,\n",
    "                email_id VARCHAR(255) UNIQUE NOT NULL,\n",
    "                contact_no VARCHAR(100) NOT NULL\n",
    "            )\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS Transactions (\n",
    "                transaction_id SERIAL PRIMARY KEY,\n",
    "                customer_id INT NOT NULL,\n",
    "                schedule_id INT NOT NULL,\n",
    "                seats_booked INT NOT NULL CHECK (seats_booked > 0),\n",
    "                ticket_price DECIMAL(5,2) NOT NULL,\n",
    "                food_amount DECIMAL(5,2) NOT NULL DEFAULT 0,\n",
    "                total_amount DECIMAL(6,2) NOT NULL,\n",
    "                booking_date TIMESTAMP NOT NULL,\n",
    "                FOREIGN KEY (customer_id) REFERENCES Customers(customer_id) ON DELETE CASCADE,\n",
    "                FOREIGN KEY (schedule_id) REFERENCES Screening_Schedule(schedule_id) ON DELETE CASCADE\n",
    "            )\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS Food_Details (\n",
    "                food_id SERIAL PRIMARY KEY,\n",
    "                transaction_id INT NOT NULL,\n",
    "                food_name VARCHAR(100) NOT NULL,\n",
    "                price DECIMAL(5,2) NOT NULL CHECK (price > 0),\n",
    "                FOREIGN KEY (transaction_id) REFERENCES Transactions(transaction_id) ON DELETE CASCADE\n",
    "            )\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS Promotions (\n",
    "                promo_id SERIAL PRIMARY KEY,\n",
    "                transaction_id INT NOT NULL,\n",
    "                promo_code VARCHAR(50) NOT NULL,\n",
    "                discount_applied DECIMAL(5,2) NOT NULL CHECK (discount_applied > 0),\n",
    "                FOREIGN KEY (transaction_id) REFERENCES Transactions(transaction_id) ON DELETE CASCADE\n",
    "            )\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS Theatre_Staff (\n",
    "                staff_id SERIAL PRIMARY KEY,\n",
    "                theatre_id INT NOT NULL,\n",
    "                name VARCHAR(255) NOT NULL,\n",
    "                role VARCHAR(50) NOT NULL,\n",
    "                salary DECIMAL(8,2) NOT NULL CHECK (salary > 0),\n",
    "                contact_no VARCHAR(100) NOT NULL,\n",
    "                email VARCHAR(255) UNIQUE NOT NULL,\n",
    "                FOREIGN KEY (theatre_id) REFERENCES Theatres(theatre_id) ON DELETE CASCADE\n",
    "            )\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS Daily_Shifts (\n",
    "                shift_id SERIAL PRIMARY KEY,\n",
    "                staff_id INT NOT NULL,\n",
    "                shift_date DATE NOT NULL,\n",
    "                shift_start_time TIME NOT NULL,\n",
    "                shift_end_time TIME NOT NULL,\n",
    "                FOREIGN KEY (staff_id) REFERENCES Theatre_Staff(staff_id) ON DELETE CASCADE\n",
    "            )\n",
    "            \"\"\"\n",
    "        ]\n",
    "        \n",
    "        # Execute table creation queries\n",
    "        for table_query in TABLES:\n",
    "            cursor.execute(table_query)\n",
    "        \n",
    "        print(\"All tables created successfully!\")\n",
    "        \n",
    "        # Commit changes and close connection\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error creating tables: {e}\")\n",
    "\n",
    "# Run the functions\n",
    "create_database()\n",
    "create_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a62c8e23-c650-476f-be53-05d22e3b547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV files with correct ID mappings and relationships!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load original CSV files\n",
    "movies_df = pd.read_csv(\"Movies.csv\")\n",
    "theaters_df = pd.read_csv(\"Theatres.csv\")\n",
    "screens_df = pd.read_csv(\"Screens.csv\")\n",
    "schedule_df = pd.read_csv(\"Screening_Schedule.csv\")\n",
    "transactions_df = pd.read_csv(\"Transactions.csv\")\n",
    "staff_df = pd.read_csv(\"Theatre_Staff.csv\")\n",
    "shifts_df = pd.read_csv(\"Daily_Shifts.csv\")\n",
    "food_df = pd.read_csv(\"Food_Details.csv\")\n",
    "promotions_df = pd.read_csv(\"Promotions.csv\")\n",
    "customers_df = pd.read_csv(\"Customers.csv\")\n",
    "\n",
    "# Assign new auto-incremented IDs while maintaining relationships\n",
    "movies_df[\"new_movie_id\"] = range(1, len(movies_df) + 1)\n",
    "theaters_df[\"new_theatre_id\"] = range(1, len(theaters_df) + 1)\n",
    "screens_df[\"new_screen_id\"] = range(1, len(screens_df) + 1)\n",
    "schedule_df[\"new_schedule_id\"] = range(1, len(schedule_df) + 1)\n",
    "transactions_df[\"new_transaction_id\"] = range(1, len(transactions_df) + 1)\n",
    "staff_df[\"new_staff_id\"] = range(1, len(staff_df) + 1)\n",
    "shifts_df[\"new_shift_id\"] = range(1, len(shifts_df) + 1)\n",
    "customers_df[\"new_customer_id\"] = range(1, len(customers_df) + 1)\n",
    "food_df[\"new_food_id\"] = range(1, len(food_df) + 1)\n",
    "promotions_df[\"new_promo_id\"] = range(1, len(promotions_df) + 1)\n",
    "\n",
    "# Create mapping dictionaries to update foreign keys\n",
    "movie_id_map = movies_df.set_index(\"movie_id\")[\"new_movie_id\"].to_dict()\n",
    "theatre_id_map = theaters_df.set_index(\"theatre_id\")[\"new_theatre_id\"].to_dict()\n",
    "screen_id_map = screens_df.set_index(\"screen_id\")[\"new_screen_id\"].to_dict()\n",
    "schedule_id_map = schedule_df.set_index(\"schedule_id\")[\"new_schedule_id\"].to_dict()\n",
    "transaction_id_map = transactions_df.set_index(\"transaction_id\")[\"new_transaction_id\"].to_dict()\n",
    "staff_id_map = staff_df.set_index(\"staff_id\")[\"new_staff_id\"].to_dict()\n",
    "customer_id_map = customers_df.set_index(\"customer_id\")[\"new_customer_id\"].to_dict()\n",
    "\n",
    "# Update foreign keys in related tables\n",
    "screens_df[\"theatre_id\"] = screens_df[\"theatre_id\"].map(theatre_id_map)\n",
    "schedule_df[\"screen_id\"] = schedule_df[\"screen_id\"].map(screen_id_map)\n",
    "schedule_df[\"movie_id\"] = schedule_df[\"movie_id\"].map(movie_id_map)\n",
    "transactions_df[\"schedule_id\"] = transactions_df[\"schedule_id\"].map(schedule_id_map)\n",
    "transactions_df[\"customer_id\"] = transactions_df[\"customer_id\"].map(customer_id_map)\n",
    "staff_df[\"theatre_id\"] = staff_df[\"theatre_id\"].map(theatre_id_map)\n",
    "shifts_df[\"staff_id\"] = shifts_df[\"staff_id\"].map(staff_id_map)\n",
    "food_df[\"transaction_id\"] = food_df[\"transaction_id\"].map(transaction_id_map)\n",
    "promotions_df[\"transaction_id\"] = promotions_df[\"transaction_id\"].map(transaction_id_map)\n",
    "\n",
    "# Drop old ID columns and rename new IDs\n",
    "movies_df.drop(columns=[\"movie_id\"], inplace=True)\n",
    "movies_df.rename(columns={\"new_movie_id\": \"movie_id\"}, inplace=True)\n",
    "\n",
    "theaters_df.drop(columns=[\"theatre_id\"], inplace=True)\n",
    "theaters_df.rename(columns={\"new_theatre_id\": \"theatre_id\"}, inplace=True)\n",
    "\n",
    "screens_df.drop(columns=[\"screen_id\"], inplace=True)\n",
    "screens_df.rename(columns={\"new_screen_id\": \"screen_id\"}, inplace=True)\n",
    "\n",
    "schedule_df.drop(columns=[\"schedule_id\"], inplace=True)\n",
    "schedule_df.rename(columns={\"new_schedule_id\": \"schedule_id\"}, inplace=True)\n",
    "\n",
    "transactions_df.drop(columns=[\"transaction_id\"], inplace=True)\n",
    "transactions_df.rename(columns={\"new_transaction_id\": \"transaction_id\"}, inplace=True)\n",
    "\n",
    "staff_df.drop(columns=[\"staff_id\"], inplace=True)\n",
    "staff_df.rename(columns={\"new_staff_id\": \"staff_id\"}, inplace=True)\n",
    "\n",
    "shifts_df.drop(columns=[\"shift_id\"], inplace=True)\n",
    "shifts_df.rename(columns={\"new_shift_id\": \"shift_id\"}, inplace=True)\n",
    "\n",
    "customers_df.drop(columns=[\"customer_id\"], inplace=True)\n",
    "customers_df.rename(columns={\"new_customer_id\": \"customer_id\"}, inplace=True)\n",
    "\n",
    "# food_df.drop(columns=[\"food_id\"], inplace=True)\n",
    "# food_df.rename(columns={\"new_food_id\": \"food_id\"}, inplace=True)\n",
    "\n",
    "# promotions_df.drop(columns=[\"promo_id\"], inplace=True)\n",
    "# promotions_df.rename(columns={\"new_promo_id\": \"promo_id\"}, inplace=True)\n",
    "\n",
    "# Save the updated CSV files\n",
    "movies_df.to_csv(\"Movies_Updated.csv\", index=False)\n",
    "theaters_df.to_csv(\"Theatres_Updated.csv\", index=False)\n",
    "screens_df.to_csv(\"Screens_Updated.csv\", index=False)\n",
    "schedule_df.to_csv(\"Screening_Schedule_Updated.csv\", index=False)\n",
    "transactions_df.to_csv(\"Transactions_Updated.csv\", index=False)\n",
    "staff_df.to_csv(\"Theatre_Staff_Updated.csv\", index=False)\n",
    "shifts_df.to_csv(\"Daily_Shifts_Updated.csv\", index=False)\n",
    "customers_df.to_csv(\"Customers_Updated.csv\", index=False)\n",
    "food_df.to_csv(\"Food_Details_Updated.csv\", index=False)\n",
    "promotions_df.to_csv(\"Promotions_Updated.csv\", index=False)\n",
    "\n",
    "print(\"Updated CSV files with correct ID mappings and relationships!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3223a7-6b97-4e9f-8477-9a04a533b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def validate_and_insert_data():\n",
    "    try:\n",
    "        # Connect to the PostgreSQL database\n",
    "        connection = psycopg2.connect(\n",
    "            dbname='movie_chain_db',\n",
    "            user='postgres',\n",
    "            password='NbKhArDy369@',\n",
    "            host='localhost',\n",
    "            port='5432'\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Load data from updated CSV files\n",
    "        movies_df = pd.read_csv(\"Movies_Updated.csv\")\n",
    "        theaters_df = pd.read_csv(\"Theatres_Updated.csv\")\n",
    "        screens_df = pd.read_csv(\"Screens_Updated.csv\")\n",
    "        schedule_df = pd.read_csv(\"Screening_Schedule_Updated.csv\")\n",
    "        transactions_df = pd.read_csv(\"Transactions_Updated.csv\")\n",
    "        staff_df = pd.read_csv(\"Theatre_Staff_Updated.csv\")\n",
    "        shifts_df = pd.read_csv(\"Daily_Shifts_Updated.csv\")\n",
    "        customers_df = pd.read_csv(\"Customers_Updated.csv\")\n",
    "        food_df = pd.read_csv(\"Food_Details_Updated.csv\")\n",
    "        promotions_df = pd.read_csv(\"Promotions_Updated.csv\")\n",
    "        \n",
    "        # Data validation and constraint enforcement\n",
    "        movies_df[\"run_time\"] = movies_df[\"run_time\"].apply(lambda x: x if x > 0 else 90)\n",
    "        screens_df[\"capacity\"] = screens_df[\"capacity\"].apply(lambda x: x if x > 0 else 50)\n",
    "        schedule_df[\"available_seats\"] = schedule_df[\"available_seats\"].apply(lambda x: x if x >= 0 else 0)\n",
    "        transactions_df[\"seats_booked\"] = transactions_df[\"seats_booked\"].apply(lambda x: x if x > 0 else 1)\n",
    "        food_df[\"price\"] = food_df[\"price\"].apply(lambda x: x if x > 0 else 5.0)\n",
    "        promotions_df[\"discount_applied\"] = promotions_df[\"discount_applied\"].apply(lambda x: x if x > 0 else 0.0)\n",
    "        staff_df[\"salary\"] = staff_df[\"salary\"].apply(lambda x: x if x > 0 else 30000)\n",
    "        \n",
    "        # Insert data into Movies\n",
    "        for _, row in movies_df.iterrows():\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO Movies (movie_id, name, rating, production_house, release_date, run_time)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\"\"\",\n",
    "                (row['movie_id'], row['name'], row['rating'], row['production_house'], row['release_date'], row['run_time'])\n",
    "            )\n",
    "        \n",
    "        # Insert data into Theatres\n",
    "        \n",
    "        \n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error inserting data into PostgreSQL: {e}\")\n",
    "\n",
    "# Run the function\n",
    "validate_and_insert_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e174efbe-ddcf-42d9-a991-0a20cb040231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Daily_Shifts_Updated.csv with correct column order and names.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def adjust_csv_columns():\n",
    "    # Define the correct column order as per SQL tables\n",
    "    column_orders = {\n",
    "        # \"Movies_Updated.csv\": [\"movie_id\", \"name\", \"rating\", \"production_house\", \"release_date\", \"run_time\"],\n",
    "        # \"Theatres_Updated.csv\": [\"theatre_id\", \"name\", \"address\", \"city\", \"state\", \"zip\", \"latitude\", \"longitude\"],\n",
    "        # \"Screens_Updated.csv\": [\"screen_id\", \"theatre_id\", \"screen_no\", \"screen_type\", \"capacity\"],\n",
    "        # \"Screening_Schedule_Updated.csv\": [\"schedule_id\", \"screen_id\", \"movie_id\", \"show_date\", \"show_time\", \"available_seats\"],\n",
    "        # \"Customers_Updated.csv\": [\"customer_id\", \"customer_name\", \"email_id\", \"contact_no\"],\n",
    "        # \"Transactions_Updated.csv\": [\"transaction_id\", \"customer_id\", \"schedule_id\", \"seats_booked\", \"ticket_price\", \"food_amount\", \"total_amount\", \"booking_date\"],\n",
    "        \n",
    "        # \"Food_Details_Updated.csv\": [\"food_id\", \"transaction_id\", \"food_name\", \"price\"],\n",
    "        # \"Promotions_Updated.csv\": [\"promo_id\", \"transaction_id\", \"promo_code\", \"discount_applied\"],\n",
    "        # transaction_id\tpromo_code\tdiscount_applied\tnew_promo_id\n",
    "        # \"Theatre_Staff_Updated.csv\": [\"staff_id\", \"theatre_id\", \"name\", \"role\", \"salary\", \"contact_no\", \"email\"],\n",
    "        # \"Daily_Shifts_Updated.csv\": [\"shift_id\", \"staff_id\", \"shift_date\", \"shift_start_time\", \"shift_end_time\"]\n",
    "    }\n",
    "    \n",
    "    for file_name, correct_order in column_orders.items():\n",
    "        try:\n",
    "            # Read CSV file\n",
    "            df = pd.read_csv(file_name)\n",
    "            # df = df.rename(columns={\"new_promo_id\": \"promo_id\"})\n",
    "            \n",
    "            # Adjust column names if different from the expected ones\n",
    "            df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]  # Normalize column names\n",
    "            correct_order_lower = [col.lower() for col in correct_order]\n",
    "            \n",
    "            # Reorder columns according to the table\n",
    "            df = df[correct_order]\n",
    "            \n",
    "            # Save updated CSV\n",
    "            df.to_csv(file_name, index=False)\n",
    "            print(f\"Updated {file_name} with correct column order and names.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# Run the function\n",
    "adjust_csv_columns()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92c726d2-3e69-417f-8e2c-5b985a3c82f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully converted: Movies_Formatted.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/7fmrypvd2mj6rc45kq6kv2lh0000gn/T/ipykernel_92651/4011215736.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\").dt.strftime('%Y-%m-%d')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_csv_to_pgadmin_txt(csv_file_path, output_txt_path):\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Ensure proper data formatting\n",
    "        if \"movie_id\" in df.columns:\n",
    "            df[\"movie_id\"] = pd.to_numeric(df[\"movie_id\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "        \n",
    "        if \"rating\" in df.columns:\n",
    "            df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\")\n",
    "        \n",
    "        if \"run_time\" in df.columns:\n",
    "            df[\"run_time\"] = pd.to_numeric(df[\"run_time\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "        \n",
    "        if \"release_date\" in df.columns:\n",
    "            df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\").dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Save to a text file with tab delimiters (better for pgAdmin import)\n",
    "        df.to_csv(output_txt_path, sep='\\t', index=False, header=True, encoding=\"utf-8\")\n",
    "        \n",
    "        print(f\"File successfully converted: {output_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "csv_file_path = \"Movies_Updated.csv\"  # Change this to your actual file path\n",
    "output_txt_path = \"Movies_Formatted.txt\"\n",
    "convert_csv_to_pgadmin_txt(csv_file_path, output_txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d543c76e-20c5-4a84-9569-008377e5e743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from Movies_Updated.csv inserted into movies successfully!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "def insert_csv_to_pgsql(csv_file_path, table_name):\n",
    "    try:\n",
    "        # Connect to PostgreSQL database\n",
    "        connection = psycopg2.connect(\n",
    "            dbname='movie_chain_db',\n",
    "            user='postgres',\n",
    "            password='NbKhArDy369@',\n",
    "            host='localhost',\n",
    "            port='5432'\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Load CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        df[\"run_time\"] = df[\"run_time\"].apply(lambda x: x if x > 0 else 90)\n",
    "        # screens_df[\"capacity\"] = screens_df[\"capacity\"].apply(lambda x: x if x > 0 else 50)\n",
    "        # schedule_df[\"available_seats\"] = schedule_df[\"available_seats\"].apply(lambda x: x if x >= 0 else 0)\n",
    "        # transactions_df[\"seats_booked\"] = transactions_df[\"seats_booked\"].apply(lambda x: x if x > 0 else 1)\n",
    "        # food_df[\"price\"] = food_df[\"price\"].apply(lambda x: x if x > 0 else 5.0)\n",
    "        # promotions_df[\"discount_applied\"] = promotions_df[\"discount_applied\"].apply(lambda x: x if x > 0 else 0.0)\n",
    "        # staff_df[\"salary\"] = staff_df[\"salary\"].apply(lambda x: x if x > 0 else 30000)\n",
    "\n",
    "        # Ensure column names match database schema\n",
    "        df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "        # Convert all NumPy data types to native Python types\n",
    "        df = df.astype(object)\n",
    "\n",
    "        # Convert dataframe to list of tuples for batch insertion\n",
    "        records_list = df.to_records(index=False).tolist()\n",
    "\n",
    "        # Create SQL query dynamically\n",
    "        columns = \", \".join(df.columns)\n",
    "        placeholders = \", \".join([\"%s\"] * len(df.columns))\n",
    "        query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "        # Use executemany for batch insert\n",
    "        cursor.executemany(query, records_list)\n",
    "\n",
    "        # Commit transaction\n",
    "        connection.commit()\n",
    "        print(f\"Data from {csv_file_path} inserted into {table_name} successfully!\")\n",
    "\n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data into {table_name}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "insert_csv_to_pgsql(\"Movies_Updated.csv\", \"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "257700d9-8260-40d4-8632-e0f011a5d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from Movies_Updated.csv inserted into movies successfully!\n",
      "Data from Theatres_Updated.csv inserted into theatres successfully!\n",
      "Data from Screens_Updated.csv inserted into screens successfully!\n",
      "Data from Screening_Schedule_Updated.csv inserted into screening_schedule successfully!\n",
      "Data from Customers_Updated.csv inserted into customers successfully!\n",
      "Data from Transactions_Updated.csv inserted into transactions successfully!\n",
      "Data from Food_Details_Updated.csv inserted into food_details successfully!\n",
      "Data from Promotions_Updated.csv inserted into promotions successfully!\n",
      "Error inserting data into theatre_staff: duplicate key value violates unique constraint \"theatre_staff_email_key\"\n",
      "DETAIL:  Key (email)=(greenjennifer@example.net) already exists.\n",
      "\n",
      "Error inserting data into daily_shifts: insert or update on table \"daily_shifts\" violates foreign key constraint \"daily_shifts_staff_id_fkey\"\n",
      "DETAIL:  Key (staff_id)=(1) is not present in table \"theatre_staff\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "def insert_csv_to_pgsql(csv_file_path, table_name, constraints=None):\n",
    "    try:\n",
    "        # Connect to PostgreSQL database\n",
    "        connection = psycopg2.connect(\n",
    "            dbname='movie_chain_db3',\n",
    "            user='postgres',\n",
    "            password='NbKhArDy369@',\n",
    "            host='localhost',\n",
    "            port='5432'\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Load CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Ensure column names match database schema\n",
    "        df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "        # Convert all NumPy data types to native Python types\n",
    "        df = df.astype(object)\n",
    "\n",
    "        # Apply constraints if provided\n",
    "        if constraints:\n",
    "            for column, condition in constraints.items():\n",
    "                df[column] = df[column].apply(condition)\n",
    "        \n",
    "        # Convert dataframe to list of tuples for batch insertion\n",
    "        records_list = df.to_records(index=False).tolist()\n",
    "\n",
    "        # Create SQL query dynamically\n",
    "        columns = \", \".join(df.columns)\n",
    "        placeholders = \", \".join([\"%s\"] * len(df.columns))\n",
    "        query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "        # Use executemany for batch insert\n",
    "        cursor.executemany(query, records_list)\n",
    "\n",
    "        # Commit transaction\n",
    "        connection.commit()\n",
    "        print(f\"Data from {csv_file_path} inserted into {table_name} successfully!\")\n",
    "\n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data into {table_name}: {e}\")\n",
    "\n",
    "# Define constraints for each table\n",
    "constraints_dict = {\n",
    "    \"movies\": {\n",
    "        \"run_time\": lambda x: x if x > 0 else 90\n",
    "    },\n",
    "    \"screens\": {\n",
    "        \"capacity\": lambda x: x if x > 0 else 50\n",
    "    },\n",
    "    \"screening_schedule\": {\n",
    "        \"available_seats\": lambda x: x if x >= 0 else 0\n",
    "    },\n",
    "    \"transactions\": {\n",
    "        \"seats_booked\": lambda x: x if x > 0 else 1\n",
    "    },\n",
    "    \"food_details\": {\n",
    "        \"price\": lambda x: x if x > 0 else 5.0\n",
    "    },\n",
    "    \"promotions\": {\n",
    "        \"discount_applied\": lambda x: x if x > 0 else 0.0\n",
    "    },\n",
    "    \"theatre_staff\": {\n",
    "        \"salary\": lambda x: x if x > 0 else 30000\n",
    "    }\n",
    "}\n",
    "\n",
    "# Insert data for all tables\n",
    "insert_csv_to_pgsql(\"Movies_Updated.csv\", \"movies\", constraints_dict.get(\"movies\"))\n",
    "insert_csv_to_pgsql(\"Theatres_Updated.csv\", \"theatres\")\n",
    "insert_csv_to_pgsql(\"Screens_Updated.csv\", \"screens\", constraints_dict.get(\"screens\"))\n",
    "insert_csv_to_pgsql(\"Screening_Schedule_Updated.csv\", \"screening_schedule\", constraints_dict.get(\"screening_schedule\"))\n",
    "insert_csv_to_pgsql(\"Customers_Updated.csv\", \"customers\")\n",
    "insert_csv_to_pgsql(\"Transactions_Updated.csv\", \"transactions\", constraints_dict.get(\"transactions\"))\n",
    "insert_csv_to_pgsql(\"Food_Details_Updated.csv\", \"food_details\", constraints_dict.get(\"food_details\"))\n",
    "insert_csv_to_pgsql(\"Promotions_Updated.csv\", \"promotions\", constraints_dict.get(\"promotions\"))\n",
    "insert_csv_to_pgsql(\"Theatre_Staff_Updated.csv\", \"theatre_staff\", constraints_dict.get(\"theatre_staff\"))\n",
    "insert_csv_to_pgsql(\"Daily_Shifts_Updated.csv\", \"daily_shifts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "80643fd5-eda0-41ee-a2ae-fb23913b851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from Theatre_Staff_Updated.csv inserted into theatre_staff successfully, duplicates removed!\n",
      "Error inserting data into daily_shifts: insert or update on table \"daily_shifts\" violates foreign key constraint \"daily_shifts_staff_id_fkey\"\n",
      "DETAIL:  Key (staff_id)=(181) is not present in table \"theatre_staff\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "def remove_duplicates(df, unique_columns):\n",
    "    \"\"\"Remove duplicate rows based on unique constraints\"\"\"\n",
    "    return df.drop_duplicates(subset=unique_columns, keep='first')\n",
    "\n",
    "def insert_csv_to_pgsql(csv_file_path, table_name, unique_columns=None, constraints=None):\n",
    "    try:\n",
    "        # Connect to PostgreSQL database\n",
    "        connection = psycopg2.connect(\n",
    "            dbname='movie_chain_db3',\n",
    "            user='postgres',\n",
    "            password='NbKhArDy369@',\n",
    "            host='localhost',\n",
    "            port='5432'\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Load CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Ensure column names match database schema\n",
    "        df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "        # Convert all NumPy data types to native Python types\n",
    "        df = df.astype(object)\n",
    "\n",
    "        # Apply constraints if provided\n",
    "        if constraints:\n",
    "            for column, condition in constraints.items():\n",
    "                df[column] = df[column].apply(condition)\n",
    "        \n",
    "        # Remove duplicates based on unique constraints\n",
    "        if unique_columns:\n",
    "            df = remove_duplicates(df, unique_columns)\n",
    "        \n",
    "        # Convert dataframe to list of tuples for batch insertion\n",
    "        records_list = df.to_records(index=False).tolist()\n",
    "\n",
    "        # Create SQL query dynamically\n",
    "        columns = \", \".join(df.columns)\n",
    "        placeholders = \", \".join([\"%s\"] * len(df.columns))\n",
    "        query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders}) ON CONFLICT DO NOTHING\"\n",
    "\n",
    "        # Use executemany for batch insert\n",
    "        cursor.executemany(query, records_list)\n",
    "\n",
    "        # Commit transaction\n",
    "        connection.commit()\n",
    "        print(f\"Data from {csv_file_path} inserted into {table_name} successfully, duplicates removed!\")\n",
    "\n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data into {table_name}: {e}\")\n",
    "\n",
    "# Define constraints and unique columns for each table\n",
    "constraints_dict = {\n",
    "    \"movies\": {\n",
    "        \"run_time\": lambda x: x if x > 0 else 90\n",
    "    },\n",
    "    \"screens\": {\n",
    "        \"capacity\": lambda x: x if x > 0 else 50\n",
    "    },\n",
    "    \"screening_schedule\": {\n",
    "        \"available_seats\": lambda x: x if x >= 0 else 0\n",
    "    },\n",
    "    \"transactions\": {\n",
    "        \"seats_booked\": lambda x: x if x > 0 else 1\n",
    "    },\n",
    "    \"food_details\": {\n",
    "        \"price\": lambda x: x if x > 0 else 5.0\n",
    "    },\n",
    "    \"promotions\": {\n",
    "        \"discount_applied\": lambda x: x if x > 0 else 0.0\n",
    "    },\n",
    "    \"theatre_staff\": {\n",
    "        \"salary\": lambda x: x if x > 0 else 30000\n",
    "    }\n",
    "}\n",
    "\n",
    "unique_columns_dict = {\n",
    "    \"theatre_staff\": [\"email\"],\n",
    "    \"daily_shifts\": [\"staff_id\", \"shift_date\"]\n",
    "}\n",
    "\n",
    "# Insert data for all tables with duplicate handling\n",
    "# insert_csv_to_pgsql(\"Movies_Updated.csv\", \"movies\", constraints_dict.get(\"movies\"))\n",
    "# insert_csv_to_pgsql(\"Theatres_Updated.csv\", \"theatres\")\n",
    "# insert_csv_to_pgsql(\"Screens_Updated.csv\", \"screens\", constraints_dict.get(\"screens\"))\n",
    "# insert_csv_to_pgsql(\"Screening_Schedule_Updated.csv\", \"screening_schedule\", constraints_dict.get(\"screening_schedule\"))\n",
    "# insert_csv_to_pgsql(\"Customers_Updated.csv\", \"customers\")\n",
    "# insert_csv_to_pgsql(\"Transactions_Updated.csv\", \"transactions\", constraints_dict.get(\"transactions\"))\n",
    "# insert_csv_to_pgsql(\"Food_Details_Updated.csv\", \"food_details\", constraints_dict.get(\"food_details\"))\n",
    "# insert_csv_to_pgsql(\"Promotions_Updated.csv\", \"promotions\", constraints_dict.get(\"promotions\"))\n",
    "insert_csv_to_pgsql(\"Theatre_Staff_Updated.csv\", \"theatre_staff\", unique_columns_dict.get(\"theatre_staff\"), constraints_dict.get(\"theatre_staff\"))\n",
    "insert_csv_to_pgsql(\"Daily_Shifts_Updated.csv\", \"daily_shifts\", unique_columns_dict.get(\"daily_shifts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "87980ab8-b037-4f13-9c55-617aad8da82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved: Theatre_Staff_Cleaned.csv\n",
      "Data from Theatre_Staff_Cleaned.csv inserted into theatre_staff successfully, duplicates handled!\n",
      "Data from Daily_Shifts_Updated.csv inserted into daily_shifts successfully, duplicates handled!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_unique_email(existing_emails, base_email):\n",
    "    \"\"\"Generate a new unique email by appending a random string.\"\"\"\n",
    "    while base_email in existing_emails:\n",
    "        random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))\n",
    "        base_email = base_email.split('@')[0] + random_suffix + \"@example.com\"\n",
    "    existing_emails.add(base_email)\n",
    "    return base_email\n",
    "\n",
    "def clean_and_update_csv(csv_file_path, unique_columns, save_path):\n",
    "    \"\"\"Modify CSV file to remove duplicates and ensure unique constraints.\"\"\"\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Ensure column names match schema\n",
    "    df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "    \n",
    "    # Handle duplicate emails for theatre_staff\n",
    "    if \"email\" in df.columns:\n",
    "        existing_emails = set()\n",
    "        df[\"email\"] = df[\"email\"].apply(lambda email: generate_unique_email(existing_emails, email))\n",
    "    \n",
    "    # Remove duplicate rows based on unique constraints\n",
    "    df = df.drop_duplicates(subset=unique_columns, keep='first')\n",
    "    \n",
    "    # Save the cleaned CSV file\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Updated CSV saved: {save_path}\")\n",
    "\n",
    "def insert_csv_to_pgsql(csv_file_path, table_name, unique_columns=None, constraints=None):\n",
    "    try:\n",
    "        # Connect to PostgreSQL database\n",
    "        connection = psycopg2.connect(\n",
    "            dbname='movie_chain_db3',\n",
    "            user='postgres',\n",
    "            password='NbKhArDy369@',\n",
    "            host='localhost',\n",
    "            port='5432'\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Load CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Ensure column names match database schema\n",
    "        df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "        \n",
    "        # Convert all NumPy data types to native Python types\n",
    "        df = df.astype(object)\n",
    "        \n",
    "        # Apply constraints if provided\n",
    "        if constraints:\n",
    "            for column, condition in constraints.items():\n",
    "                df[column] = df[column].apply(condition)\n",
    "        \n",
    "        # Convert dataframe to list of tuples for batch insertion\n",
    "        records_list = df.to_records(index=False).tolist()\n",
    "        \n",
    "        # Create SQL query dynamically\n",
    "        columns = \", \".join(df.columns)\n",
    "        placeholders = \", \".join([\"%s\"] * len(df.columns))\n",
    "        query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders}) ON CONFLICT DO NOTHING\"\n",
    "        \n",
    "        # Use executemany for batch insert\n",
    "        cursor.executemany(query, records_list)\n",
    "        \n",
    "        # Commit transaction\n",
    "        connection.commit()\n",
    "        print(f\"Data from {csv_file_path} inserted into {table_name} successfully, duplicates handled!\")\n",
    "        \n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data into {table_name}: {e}\")\n",
    "\n",
    "# Define constraints and unique columns for each table\n",
    "constraints_dict = {\n",
    "    \"movies\": {\n",
    "        \"run_time\": lambda x: x if x > 0 else 90\n",
    "    },\n",
    "    \"screens\": {\n",
    "        \"capacity\": lambda x: x if x > 0 else 50\n",
    "    },\n",
    "    \"screening_schedule\": {\n",
    "        \"available_seats\": lambda x: x if x >= 0 else 0\n",
    "    },\n",
    "    \"transactions\": {\n",
    "        \"seats_booked\": lambda x: x if x > 0 else 1\n",
    "    },\n",
    "    \"food_details\": {\n",
    "        \"price\": lambda x: x if x > 0 else 5.0\n",
    "    },\n",
    "    \"promotions\": {\n",
    "        \"discount_applied\": lambda x: x if x > 0 else 0.0\n",
    "    },\n",
    "    \"theatre_staff\": {\n",
    "        \"salary\": lambda x: x if x > 0 else 30000\n",
    "    }\n",
    "}\n",
    "\n",
    "unique_columns_dict = {\n",
    "    \"theatre_staff\": [\"email\"],\n",
    "    \"daily_shifts\": [\"staff_id\", \"shift_date\"]\n",
    "}\n",
    "\n",
    "# Clean and update theatre_staff CSV before inserting\n",
    "clean_and_update_csv(\"Theatre_Staff_Updated.csv\", unique_columns_dict.get(\"theatre_staff\"), \"Theatre_Staff_Cleaned.csv\")\n",
    "\n",
    "# Insert data for all tables with duplicate handling\n",
    "# insert_csv_to_pgsql(\"Movies_Updated.csv\", \"movies\", constraints_dict.get(\"movies\"))\n",
    "# insert_csv_to_pgsql(\"Theatres_Updated.csv\", \"theatres\")\n",
    "# insert_csv_to_pgsql(\"Screens_Updated.csv\", \"screens\", constraints_dict.get(\"screens\"))\n",
    "# insert_csv_to_pgsql(\"Screening_Schedule_Updated.csv\", \"screening_schedule\", constraints_dict.get(\"screening_schedule\"))\n",
    "# insert_csv_to_pgsql(\"Customers_Updated.csv\", \"customers\")\n",
    "# insert_csv_to_pgsql(\"Transactions_Updated.csv\", \"transactions\", constraints_dict.get(\"transactions\"))\n",
    "# insert_csv_to_pgsql(\"Food_Details_Updated.csv\", \"food_details\", constraints_dict.get(\"food_details\"))\n",
    "# insert_csv_to_pgsql(\"Promotions_Updated.csv\", \"promotions\", constraints_dict.get(\"promotions\"))\n",
    "insert_csv_to_pgsql(\"Theatre_Staff_Cleaned.csv\", \"theatre_staff\", unique_columns_dict.get(\"theatre_staff\"), constraints_dict.get(\"theatre_staff\"))\n",
    "insert_csv_to_pgsql(\"Daily_Shifts_Updated.csv\", \"daily_shifts\", unique_columns_dict.get(\"daily_shifts\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
